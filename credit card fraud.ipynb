{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "FILE_NAME = 'creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Label  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "full_data = pd.read_csv(FILE_NAME)\n",
    "\n",
    "#rename the 'Class' column\n",
    "full_data.rename(columns = {'Class': 'Label'}, inplace = True)\n",
    "\n",
    "#let's take a peek\n",
    "print full_data.shape\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consists of 284807 instances of data with 29 total features with value counts of \n",
      "0    284315\n",
      "1       492\n",
      "Name: Label, dtype: int64\n",
      "Where 0 indicates a legitimate transaction and 1 indicates fraud\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "full_data = shuffle(full_data)\n",
    "\n",
    "# Seperate target labels\n",
    "labels = full_data['Label']\n",
    "\n",
    "times = full_data['Time']\n",
    "features = full_data.drop(['Time', 'Label'], axis=1)\n",
    "\n",
    "# Get some specifics on our dataset\n",
    "print \"Data consists of {} instances of data with {} total features with value counts of \\n{}\".format(\n",
    "    features.shape[0], features.shape[1], labels.value_counts())\n",
    "print \"Where 0 indicates a legitimate transaction and 1 indicates fraud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the amount spent\n",
    "features['normAmount'] = StandardScaler().fit_transform(features['Amount'].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185792</th>\n",
       "      <td>1.397299</td>\n",
       "      <td>-1.741681</td>\n",
       "      <td>-1.702283</td>\n",
       "      <td>-0.546288</td>\n",
       "      <td>1.178657</td>\n",
       "      <td>4.095666</td>\n",
       "      <td>-1.112828</td>\n",
       "      <td>1.051522</td>\n",
       "      <td>1.153431</td>\n",
       "      <td>-0.134940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499839</td>\n",
       "      <td>0.190863</td>\n",
       "      <td>-0.081690</td>\n",
       "      <td>0.109755</td>\n",
       "      <td>0.742016</td>\n",
       "      <td>-0.672251</td>\n",
       "      <td>0.452993</td>\n",
       "      <td>-0.050097</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.854153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92080</th>\n",
       "      <td>-6.585849</td>\n",
       "      <td>5.536218</td>\n",
       "      <td>-5.384978</td>\n",
       "      <td>0.722320</td>\n",
       "      <td>-3.401588</td>\n",
       "      <td>-0.964978</td>\n",
       "      <td>-3.518547</td>\n",
       "      <td>5.100348</td>\n",
       "      <td>-0.911330</td>\n",
       "      <td>0.741915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.288105</td>\n",
       "      <td>0.430075</td>\n",
       "      <td>0.482247</td>\n",
       "      <td>0.778484</td>\n",
       "      <td>-0.314855</td>\n",
       "      <td>-0.022651</td>\n",
       "      <td>-0.347376</td>\n",
       "      <td>-0.419721</td>\n",
       "      <td>-0.100811</td>\n",
       "      <td>-0.349671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100052</th>\n",
       "      <td>-0.819556</td>\n",
       "      <td>0.749402</td>\n",
       "      <td>1.561889</td>\n",
       "      <td>1.375082</td>\n",
       "      <td>-0.135148</td>\n",
       "      <td>0.926043</td>\n",
       "      <td>-0.374834</td>\n",
       "      <td>0.838653</td>\n",
       "      <td>-0.276296</td>\n",
       "      <td>-0.486815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044613</td>\n",
       "      <td>0.118976</td>\n",
       "      <td>0.524225</td>\n",
       "      <td>-0.181750</td>\n",
       "      <td>-0.256281</td>\n",
       "      <td>-0.160029</td>\n",
       "      <td>-0.184082</td>\n",
       "      <td>0.096795</td>\n",
       "      <td>0.045204</td>\n",
       "      <td>-0.329481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253908</th>\n",
       "      <td>2.255542</td>\n",
       "      <td>-1.221431</td>\n",
       "      <td>-2.080572</td>\n",
       "      <td>-1.629280</td>\n",
       "      <td>-0.436930</td>\n",
       "      <td>-1.018379</td>\n",
       "      <td>-0.253582</td>\n",
       "      <td>-0.442473</td>\n",
       "      <td>-2.097884</td>\n",
       "      <td>1.804853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348068</td>\n",
       "      <td>0.022843</td>\n",
       "      <td>0.523847</td>\n",
       "      <td>-0.095561</td>\n",
       "      <td>-0.353712</td>\n",
       "      <td>0.378789</td>\n",
       "      <td>0.110946</td>\n",
       "      <td>-0.053256</td>\n",
       "      <td>-0.082588</td>\n",
       "      <td>-0.117582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58136</th>\n",
       "      <td>0.021365</td>\n",
       "      <td>-0.301952</td>\n",
       "      <td>1.135928</td>\n",
       "      <td>-1.124444</td>\n",
       "      <td>-0.349726</td>\n",
       "      <td>0.943416</td>\n",
       "      <td>-0.558524</td>\n",
       "      <td>0.310153</td>\n",
       "      <td>-2.823375</td>\n",
       "      <td>1.329564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261638</td>\n",
       "      <td>0.054702</td>\n",
       "      <td>0.769835</td>\n",
       "      <td>0.077042</td>\n",
       "      <td>-0.639253</td>\n",
       "      <td>-0.802412</td>\n",
       "      <td>0.123912</td>\n",
       "      <td>0.261039</td>\n",
       "      <td>0.142663</td>\n",
       "      <td>-0.344394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "185792  1.397299 -1.741681 -1.702283 -0.546288  1.178657  4.095666 -1.112828   \n",
       "92080  -6.585849  5.536218 -5.384978  0.722320 -3.401588 -0.964978 -3.518547   \n",
       "100052 -0.819556  0.749402  1.561889  1.375082 -0.135148  0.926043 -0.374834   \n",
       "253908  2.255542 -1.221431 -2.080572 -1.629280 -0.436930 -1.018379 -0.253582   \n",
       "58136   0.021365 -0.301952  1.135928 -1.124444 -0.349726  0.943416 -0.558524   \n",
       "\n",
       "              V8        V9       V10     ...           V20       V21  \\\n",
       "185792  1.051522  1.153431 -0.134940     ...      0.499839  0.190863   \n",
       "92080   5.100348 -0.911330  0.741915     ...     -0.288105  0.430075   \n",
       "100052  0.838653 -0.276296 -0.486815     ...     -0.044613  0.118976   \n",
       "253908 -0.442473 -2.097884  1.804853     ...     -0.348068  0.022843   \n",
       "58136   0.310153 -2.823375  1.329564     ...     -0.261638  0.054702   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "185792 -0.081690  0.109755  0.742016 -0.672251  0.452993 -0.050097  0.006405   \n",
       "92080   0.482247  0.778484 -0.314855 -0.022651 -0.347376 -0.419721 -0.100811   \n",
       "100052  0.524225 -0.181750 -0.256281 -0.160029 -0.184082  0.096795  0.045204   \n",
       "253908  0.523847 -0.095561 -0.353712  0.378789  0.110946 -0.053256 -0.082588   \n",
       "58136   0.769835  0.077042 -0.639253 -0.802412  0.123912  0.261039  0.142663   \n",
       "\n",
       "        normAmount  \n",
       "185792    0.854153  \n",
       "92080    -0.349671  \n",
       "100052   -0.329481  \n",
       "253908   -0.117582  \n",
       "58136    -0.344394  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts = features['Amount']\n",
    "features = features.drop(['Amount'], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    213239\n",
      "1       366\n",
      "Name: Label, dtype: int64\n",
      "0    71076\n",
      "1      126\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "# We're going to hold out a test set from oversampling to see how our model trained on the oversampled data does on the original data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = .25)\n",
    "\n",
    "print train_labels.value_counts()\n",
    "print test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# implement synthetic minority oversampling technique for a more balanced dataset to feed our model\n",
    "oversampler = SMOTE(random_state=331)\n",
    "os_features, os_labels = oversampler.fit_sample(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training instances of data: 319858\n",
      "training instances of fraud 159735\n",
      "testing instances of data: 106620\n",
      "testing instances of fraud: 53504\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(os_features, os_labels, test_size = .25)\n",
    "\n",
    "# Let's get an idea of what our new oversampled data looks like\n",
    "\n",
    "print 'training instances of data:' , len(y_train) \n",
    "print 'training instances of fraud' , np.count_nonzero(y_train)\n",
    "print 'testing instances of data:' , len(y_test) \n",
    "print 'testing instances of fraud:' , np.count_nonzero(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Random Forest Classifier:\n",
      "[ 0.99985226  0.99984522  0.99988744] 0.999861641878\n",
      "For K-Nearest Neighbors Classifier:\n",
      "[ 0.99878158  0.99897898  0.99884502] 0.998868527103\n"
     ]
    }
   ],
   "source": [
    "# Try a couple out-of-the-box classifiers to establish a benchmark\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "print \"For Random Forest Classifier:\"\n",
    "rfscores = cross_val_score(rf, os_features, os_labels, scoring = f1_scorer)\n",
    "print rfscores, rfscores.mean()\n",
    "\n",
    "print \"For K-Nearest Neighbors Classifier:\"\n",
    "knnscores = cross_val_score(knn, os_features, os_labels, scoring = f1_scorer)\n",
    "print knnscores, knnscores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for simple majority vote is  0.999135510488\n"
     ]
    }
   ],
   "source": [
    "# simple majority vote benchmark without oversampling\n",
    "majority_vote_predictions = np.zeros(features.shape[0])\n",
    "print \"f1 score for simple majority vote is \" , f1_score(labels, majority_vote_predictions, pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999902 -   3.0s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999852 -   2.5s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999909 -   2.4s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=entropy, max_features=25, score=0.999782 -   1.2s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=entropy, max_features=25, score=0.999789 -   1.1s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=entropy, max_features=25, score=0.999796 -   1.4s\n",
      "[CV] n_estimators=55, min_samples_split=4, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=4, criterion=entropy, max_features=1, score=0.999902 -   1.3s\n",
      "[CV] n_estimators=55, min_samples_split=4, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=4, criterion=entropy, max_features=1, score=0.999866 -   1.3s\n",
      "[CV] n_estimators=55, min_samples_split=4, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=4, criterion=entropy, max_features=1, score=0.999916 -   1.3s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=25, score=0.999733 -   1.6s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=25, score=0.999782 -   1.6s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=25, score=0.999789 -   1.4s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=entropy, max_features=9, score=0.999817 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=entropy, max_features=9, score=0.999817 -   0.2s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=entropy, max_features=9, score=0.999852 -   0.2s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=17, score=0.999831 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=17, score=0.999782 -   1.1s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=17, score=0.999852 -   1.1s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999866 -   1.0s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999824 -   1.0s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999902 -   1.0s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=1, score=0.999873 -   1.3s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=1, score=0.999859 -   1.3s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=1, score=0.999894 -   1.4s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=5, score=0.999852 -   1.6s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=5, score=0.999838 -   1.6s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=5, score=0.999916 -   1.6s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9, score=0.999887 -   0.7s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9, score=0.999824 -   0.7s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9, score=0.999880 -   0.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=17, score=0.999845 -   2.4s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=17, score=0.999803 -   2.1s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=17, score=0.999838 -   2.2s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=1, score=0.999880 -   2.3s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=1, score=0.999845 -   2.4s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=1, score=0.999916 -   2.3s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1, score=0.999866 -   2.0s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1, score=0.999831 -   2.0s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1, score=0.999923 -   2.1s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999859 -   1.4s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999852 -   1.3s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999923 -   1.3s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=5, score=0.999880 -   2.1s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=5, score=0.999845 -   1.9s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=5, score=0.999916 -   1.9s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=9, score=0.999845 -   0.8s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=9, score=0.999852 -   0.9s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=9, score=0.999894 -   0.8s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=5, score=0.999845 -   1.3s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=5, score=0.999859 -   1.1s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=5, score=0.999902 -   1.1s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=entropy, max_features=25, score=0.999775 -   0.9s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=entropy, max_features=25, score=0.999740 -   1.0s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=entropy, max_features=25, score=0.999761 -   0.9s\n",
      "[CV] n_estimators=25, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=25, min_samples_split=6, criterion=entropy, max_features=17, score=0.999810 -   0.4s\n",
      "[CV] n_estimators=25, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=25, min_samples_split=6, criterion=entropy, max_features=17, score=0.999775 -   0.4s\n",
      "[CV] n_estimators=25, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=25, min_samples_split=6, criterion=entropy, max_features=17, score=0.999866 -   0.4s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=1, score=0.999873 -   0.9s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=1, score=0.999859 -   0.9s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=1, score=0.999923 -   0.9s\n",
      "[CV] n_estimators=25, min_samples_split=4, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=25, min_samples_split=4, criterion=entropy, max_features=1, score=0.999852 -   0.6s\n",
      "[CV] n_estimators=25, min_samples_split=4, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=25, min_samples_split=4, criterion=entropy, max_features=1, score=0.999838 -   0.6s\n",
      "[CV] n_estimators=25, min_samples_split=4, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=25, min_samples_split=4, criterion=entropy, max_features=1, score=0.999894 -   0.5s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=13, score=0.999775 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=13, score=0.999789 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=13, score=0.999866 -   0.1s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=1, score=0.999894 -   1.0s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=1, score=0.999852 -   1.0s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=1, score=0.999902 -   1.2s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999838 -   2.3s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999831 -   1.4s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999894 -   1.4s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=21, score=0.999852 -   1.7s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=21, score=0.999831 -   1.9s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=21, score=0.999852 -   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 446.3min finished\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier looked promising, let's try to tune the hyper-parameters to get better results\n",
    "rf_params = {'n_estimators' : np.arange(10, 110, 15),\n",
    "                'min_samples_split': np.arange(2, 8, 2),\n",
    "                'max_features': np.arange(1, 29, 4),\n",
    "                'criterion': ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "rf_tune = RandomizedSearchCV(rf, rf_params, n_iter = 25, scoring = f1_scorer, verbose = 3)\n",
    "\n",
    "rf_tune = rf_tune.fit(os_features, os_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=55, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False) \n",
      "f1 score: 0.999894473029\n"
     ]
    }
   ],
   "source": [
    "print rf_tune.best_estimator_ , '\\nf1 score:' , rf_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.23680075,  -4.56654399,   4.70044703, ...,  -9.81446283,\n",
       "         -7.50218262,  -9.79137868],\n",
       "       [  3.79214682,  -5.2825743 ,   2.79352888, ...,  -6.22354782,\n",
       "         -1.92455349,  -3.53997487],\n",
       "       [  8.98284196, -23.01996283,  11.85558537, ..., -13.64830977,\n",
       "         -7.91042923, -11.88942851],\n",
       "       ..., \n",
       "       [  0.9227921 ,   1.35322822,   2.46156735, ...,  -0.27296682,\n",
       "          0.18609665,  -0.28561962],\n",
       "       [  1.85965455,  -5.90800958,   2.03285475, ...,  -7.26265411,\n",
       "         -3.8487812 ,  -5.25077537],\n",
       "       [  1.24173691,  -1.43606486,   2.2338707 , ...,  -4.70004828,\n",
       "         -2.67023692,  -5.65688996]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft = rf_tune.best_estimator_\n",
    "rft.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 testing score for tuned random forest on oversampled data is  0.999887026925\n",
      "f1 testing score for tuned random forest on original test data is  0.999711598658\n"
     ]
    }
   ],
   "source": [
    "# Check performance of tuned model \n",
    "# Add check on original data before oversampling\n",
    "print \"f1 testing score for tuned random forest on oversampled data is \", f1_score(y_test, rft.predict(X_test), pos_label = 0)\n",
    "print \"f1 testing score for tuned random forest on original test data is \", f1_score(test_labels, rft.predict(test_features), pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of random forest performance on oversampled test data:\n",
      "[[53104    12]\n",
      " [    0 53504]]\n",
      "Confusion matrix of random forest performance on original test data:\n",
      "[[71061    15]\n",
      " [   26   100]]\n"
     ]
    }
   ],
   "source": [
    "print \"Confusion matrix of random forest performance on oversampled test data:\"\n",
    "print confusion_matrix(y_test, rft.predict(X_test))\n",
    "print \"Confusion matrix of random forest performance on original test data:\"\n",
    "print confusion_matrix(test_labels, rft.predict(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do: discuss the results above, especially Type 1 errors on the original test data :/\n",
    "\n",
    "Now let's try a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's build a model\n",
    "\n",
    "# needs more tuning\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim = X_train.shape[1], activation = 'tanh', init = 'lecun_uniform', W_regularizer = l2(.01)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(12, activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dense(4, activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "sgd = SGD(lr = .16, momentum = .7, decay = .01)\n",
    "\n",
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 255886 samples, validate on 63972 samples\n",
      "Epoch 1/256\n",
      "2s - loss: 0.2149 - fmeasure: 0.9553 - val_loss: 0.0971 - val_fmeasure: 0.9636\n",
      "Epoch 2/256\n",
      "1s - loss: 0.1126 - fmeasure: 0.9659 - val_loss: 0.0873 - val_fmeasure: 0.9673\n",
      "Epoch 3/256\n",
      "1s - loss: 0.0999 - fmeasure: 0.9684 - val_loss: 0.0805 - val_fmeasure: 0.9677\n",
      "Epoch 4/256\n",
      "1s - loss: 0.0924 - fmeasure: 0.9710 - val_loss: 0.0739 - val_fmeasure: 0.9715\n",
      "Epoch 5/256\n",
      "1s - loss: 0.0869 - fmeasure: 0.9731 - val_loss: 0.0686 - val_fmeasure: 0.9737\n",
      "Epoch 6/256\n",
      "1s - loss: 0.0827 - fmeasure: 0.9750 - val_loss: 0.0649 - val_fmeasure: 0.9760\n",
      "Epoch 7/256\n",
      "1s - loss: 0.0796 - fmeasure: 0.9761 - val_loss: 0.0628 - val_fmeasure: 0.9761\n",
      "Epoch 8/256\n",
      "1s - loss: 0.0773 - fmeasure: 0.9773 - val_loss: 0.0606 - val_fmeasure: 0.9769\n",
      "Epoch 9/256\n",
      "1s - loss: 0.0755 - fmeasure: 0.9780 - val_loss: 0.0584 - val_fmeasure: 0.9767\n",
      "Epoch 10/256\n",
      "1s - loss: 0.0739 - fmeasure: 0.9787 - val_loss: 0.0570 - val_fmeasure: 0.9803\n",
      "Epoch 11/256\n",
      "1s - loss: 0.0726 - fmeasure: 0.9793 - val_loss: 0.0561 - val_fmeasure: 0.9798\n",
      "Epoch 12/256\n",
      "1s - loss: 0.0715 - fmeasure: 0.9798 - val_loss: 0.0550 - val_fmeasure: 0.9795\n",
      "Epoch 13/256\n",
      "1s - loss: 0.0705 - fmeasure: 0.9801 - val_loss: 0.0542 - val_fmeasure: 0.9810\n",
      "Epoch 14/256\n",
      "1s - loss: 0.0697 - fmeasure: 0.9806 - val_loss: 0.0531 - val_fmeasure: 0.9800\n",
      "Epoch 15/256\n",
      "1s - loss: 0.0688 - fmeasure: 0.9810 - val_loss: 0.0525 - val_fmeasure: 0.9810\n",
      "Epoch 16/256\n",
      "1s - loss: 0.0681 - fmeasure: 0.9812 - val_loss: 0.0519 - val_fmeasure: 0.9814\n",
      "Epoch 17/256\n",
      "1s - loss: 0.0675 - fmeasure: 0.9814 - val_loss: 0.0517 - val_fmeasure: 0.9814\n",
      "Epoch 18/256\n",
      "1s - loss: 0.0668 - fmeasure: 0.9817 - val_loss: 0.0506 - val_fmeasure: 0.9817\n",
      "Epoch 19/256\n",
      "1s - loss: 0.0663 - fmeasure: 0.9820 - val_loss: 0.0500 - val_fmeasure: 0.9818\n",
      "Epoch 20/256\n",
      "1s - loss: 0.0657 - fmeasure: 0.9821 - val_loss: 0.0500 - val_fmeasure: 0.9810\n",
      "Epoch 21/256\n",
      "1s - loss: 0.0652 - fmeasure: 0.9823 - val_loss: 0.0492 - val_fmeasure: 0.9822\n",
      "Epoch 22/256\n",
      "1s - loss: 0.0647 - fmeasure: 0.9825 - val_loss: 0.0487 - val_fmeasure: 0.9820\n",
      "Epoch 23/256\n",
      "1s - loss: 0.0643 - fmeasure: 0.9826 - val_loss: 0.0485 - val_fmeasure: 0.9834\n",
      "Epoch 24/256\n",
      "1s - loss: 0.0638 - fmeasure: 0.9828 - val_loss: 0.0482 - val_fmeasure: 0.9824\n",
      "Epoch 25/256\n",
      "1s - loss: 0.0634 - fmeasure: 0.9831 - val_loss: 0.0473 - val_fmeasure: 0.9825\n",
      "Epoch 26/256\n",
      "1s - loss: 0.0631 - fmeasure: 0.9832 - val_loss: 0.0474 - val_fmeasure: 0.9839\n",
      "Epoch 27/256\n",
      "2s - loss: 0.0627 - fmeasure: 0.9836 - val_loss: 0.0465 - val_fmeasure: 0.9828\n",
      "Epoch 28/256\n",
      "1s - loss: 0.0624 - fmeasure: 0.9837 - val_loss: 0.0470 - val_fmeasure: 0.9826\n",
      "Epoch 29/256\n",
      "1s - loss: 0.0621 - fmeasure: 0.9838 - val_loss: 0.0464 - val_fmeasure: 0.9835\n",
      "Epoch 30/256\n",
      "1s - loss: 0.0618 - fmeasure: 0.9839 - val_loss: 0.0456 - val_fmeasure: 0.9834\n",
      "Epoch 31/256\n",
      "1s - loss: 0.0615 - fmeasure: 0.9840 - val_loss: 0.0455 - val_fmeasure: 0.9836\n",
      "Epoch 32/256\n",
      "1s - loss: 0.0612 - fmeasure: 0.9843 - val_loss: 0.0453 - val_fmeasure: 0.9835\n",
      "Epoch 33/256\n",
      "2s - loss: 0.0608 - fmeasure: 0.9845 - val_loss: 0.0453 - val_fmeasure: 0.9837\n",
      "Epoch 34/256\n",
      "2s - loss: 0.0606 - fmeasure: 0.9845 - val_loss: 0.0446 - val_fmeasure: 0.9845\n",
      "Epoch 35/256\n",
      "2s - loss: 0.0603 - fmeasure: 0.9846 - val_loss: 0.0446 - val_fmeasure: 0.9846\n",
      "Epoch 36/256\n",
      "1s - loss: 0.0601 - fmeasure: 0.9850 - val_loss: 0.0442 - val_fmeasure: 0.9848\n",
      "Epoch 37/256\n",
      "1s - loss: 0.0599 - fmeasure: 0.9850 - val_loss: 0.0439 - val_fmeasure: 0.9845\n",
      "Epoch 38/256\n",
      "1s - loss: 0.0597 - fmeasure: 0.9851 - val_loss: 0.0441 - val_fmeasure: 0.9844\n",
      "Epoch 39/256\n",
      "1s - loss: 0.0594 - fmeasure: 0.9853 - val_loss: 0.0436 - val_fmeasure: 0.9849\n",
      "Epoch 40/256\n",
      "1s - loss: 0.0592 - fmeasure: 0.9855 - val_loss: 0.0433 - val_fmeasure: 0.9844\n",
      "Epoch 41/256\n",
      "1s - loss: 0.0590 - fmeasure: 0.9856 - val_loss: 0.0431 - val_fmeasure: 0.9856\n",
      "Epoch 42/256\n",
      "1s - loss: 0.0589 - fmeasure: 0.9856 - val_loss: 0.0428 - val_fmeasure: 0.9854\n",
      "Epoch 43/256\n",
      "1s - loss: 0.0586 - fmeasure: 0.9856 - val_loss: 0.0427 - val_fmeasure: 0.9855\n",
      "Epoch 44/256\n",
      "1s - loss: 0.0585 - fmeasure: 0.9858 - val_loss: 0.0429 - val_fmeasure: 0.9864\n",
      "Epoch 45/256\n",
      "1s - loss: 0.0583 - fmeasure: 0.9861 - val_loss: 0.0423 - val_fmeasure: 0.9855\n",
      "Epoch 46/256\n",
      "1s - loss: 0.0581 - fmeasure: 0.9861 - val_loss: 0.0422 - val_fmeasure: 0.9859\n",
      "Epoch 47/256\n",
      "1s - loss: 0.0579 - fmeasure: 0.9862 - val_loss: 0.0422 - val_fmeasure: 0.9862\n",
      "Epoch 48/256\n",
      "1s - loss: 0.0578 - fmeasure: 0.9862 - val_loss: 0.0418 - val_fmeasure: 0.9860\n",
      "Epoch 49/256\n",
      "1s - loss: 0.0576 - fmeasure: 0.9863 - val_loss: 0.0415 - val_fmeasure: 0.9861\n",
      "Epoch 50/256\n",
      "1s - loss: 0.0575 - fmeasure: 0.9863 - val_loss: 0.0416 - val_fmeasure: 0.9864\n",
      "Epoch 51/256\n",
      "1s - loss: 0.0574 - fmeasure: 0.9864 - val_loss: 0.0416 - val_fmeasure: 0.9863\n",
      "Epoch 52/256\n",
      "1s - loss: 0.0572 - fmeasure: 0.9867 - val_loss: 0.0413 - val_fmeasure: 0.9865\n",
      "Epoch 53/256\n",
      "1s - loss: 0.0570 - fmeasure: 0.9867 - val_loss: 0.0412 - val_fmeasure: 0.9866\n",
      "Epoch 54/256\n",
      "1s - loss: 0.0569 - fmeasure: 0.9867 - val_loss: 0.0411 - val_fmeasure: 0.9864\n",
      "Epoch 55/256\n",
      "1s - loss: 0.0568 - fmeasure: 0.9868 - val_loss: 0.0408 - val_fmeasure: 0.9866\n",
      "Epoch 56/256\n",
      "1s - loss: 0.0566 - fmeasure: 0.9866 - val_loss: 0.0408 - val_fmeasure: 0.9868\n",
      "Epoch 57/256\n",
      "1s - loss: 0.0565 - fmeasure: 0.9869 - val_loss: 0.0407 - val_fmeasure: 0.9870\n",
      "Epoch 58/256\n",
      "1s - loss: 0.0564 - fmeasure: 0.9869 - val_loss: 0.0405 - val_fmeasure: 0.9871\n",
      "Epoch 59/256\n",
      "1s - loss: 0.0562 - fmeasure: 0.9872 - val_loss: 0.0404 - val_fmeasure: 0.9870\n",
      "Epoch 60/256\n",
      "1s - loss: 0.0561 - fmeasure: 0.9870 - val_loss: 0.0402 - val_fmeasure: 0.9870\n",
      "Epoch 61/256\n",
      "1s - loss: 0.0560 - fmeasure: 0.9871 - val_loss: 0.0403 - val_fmeasure: 0.9869\n",
      "Epoch 62/256\n",
      "1s - loss: 0.0559 - fmeasure: 0.9872 - val_loss: 0.0401 - val_fmeasure: 0.9869\n",
      "Epoch 63/256\n",
      "1s - loss: 0.0558 - fmeasure: 0.9872 - val_loss: 0.0399 - val_fmeasure: 0.9869\n",
      "Epoch 64/256\n",
      "1s - loss: 0.0557 - fmeasure: 0.9873 - val_loss: 0.0398 - val_fmeasure: 0.9870\n",
      "Epoch 65/256\n",
      "1s - loss: 0.0556 - fmeasure: 0.9872 - val_loss: 0.0399 - val_fmeasure: 0.9870\n",
      "Epoch 66/256\n",
      "1s - loss: 0.0555 - fmeasure: 0.9874 - val_loss: 0.0396 - val_fmeasure: 0.9871\n",
      "Epoch 67/256\n",
      "1s - loss: 0.0554 - fmeasure: 0.9874 - val_loss: 0.0395 - val_fmeasure: 0.9874\n",
      "Epoch 68/256\n",
      "1s - loss: 0.0553 - fmeasure: 0.9874 - val_loss: 0.0394 - val_fmeasure: 0.9873\n",
      "Epoch 69/256\n",
      "1s - loss: 0.0552 - fmeasure: 0.9875 - val_loss: 0.0393 - val_fmeasure: 0.9875\n",
      "Epoch 70/256\n",
      "1s - loss: 0.0551 - fmeasure: 0.9876 - val_loss: 0.0394 - val_fmeasure: 0.9872\n",
      "Epoch 71/256\n",
      "1s - loss: 0.0550 - fmeasure: 0.9875 - val_loss: 0.0392 - val_fmeasure: 0.9875\n",
      "Epoch 72/256\n",
      "1s - loss: 0.0549 - fmeasure: 0.9876 - val_loss: 0.0391 - val_fmeasure: 0.9874\n",
      "Epoch 73/256\n",
      "1s - loss: 0.0548 - fmeasure: 0.9877 - val_loss: 0.0390 - val_fmeasure: 0.9874\n",
      "Epoch 74/256\n",
      "1s - loss: 0.0547 - fmeasure: 0.9877 - val_loss: 0.0388 - val_fmeasure: 0.9878\n",
      "Epoch 75/256\n",
      "1s - loss: 0.0546 - fmeasure: 0.9877 - val_loss: 0.0389 - val_fmeasure: 0.9874\n",
      "Epoch 76/256\n",
      "1s - loss: 0.0545 - fmeasure: 0.9878 - val_loss: 0.0389 - val_fmeasure: 0.9882\n",
      "Epoch 77/256\n",
      "1s - loss: 0.0545 - fmeasure: 0.9879 - val_loss: 0.0386 - val_fmeasure: 0.9877\n",
      "Epoch 78/256\n",
      "1s - loss: 0.0544 - fmeasure: 0.9880 - val_loss: 0.0386 - val_fmeasure: 0.9875\n",
      "Epoch 79/256\n",
      "1s - loss: 0.0543 - fmeasure: 0.9878 - val_loss: 0.0386 - val_fmeasure: 0.9876\n",
      "Epoch 80/256\n",
      "1s - loss: 0.0542 - fmeasure: 0.9879 - val_loss: 0.0385 - val_fmeasure: 0.9878\n",
      "Epoch 81/256\n",
      "1s - loss: 0.0541 - fmeasure: 0.9880 - val_loss: 0.0385 - val_fmeasure: 0.9877\n",
      "Epoch 82/256\n",
      "1s - loss: 0.0541 - fmeasure: 0.9879 - val_loss: 0.0383 - val_fmeasure: 0.9879\n",
      "Epoch 83/256\n",
      "1s - loss: 0.0540 - fmeasure: 0.9880 - val_loss: 0.0384 - val_fmeasure: 0.9883\n",
      "Epoch 84/256\n",
      "1s - loss: 0.0539 - fmeasure: 0.9880 - val_loss: 0.0382 - val_fmeasure: 0.9880\n",
      "Epoch 85/256\n",
      "1s - loss: 0.0538 - fmeasure: 0.9880 - val_loss: 0.0382 - val_fmeasure: 0.9879\n",
      "Epoch 86/256\n",
      "1s - loss: 0.0538 - fmeasure: 0.9881 - val_loss: 0.0381 - val_fmeasure: 0.9880\n",
      "Epoch 87/256\n",
      "1s - loss: 0.0537 - fmeasure: 0.9881 - val_loss: 0.0381 - val_fmeasure: 0.9881\n",
      "Epoch 88/256\n",
      "1s - loss: 0.0536 - fmeasure: 0.9882 - val_loss: 0.0382 - val_fmeasure: 0.9883\n",
      "Epoch 89/256\n",
      "1s - loss: 0.0536 - fmeasure: 0.9882 - val_loss: 0.0382 - val_fmeasure: 0.9880\n",
      "Epoch 90/256\n",
      "1s - loss: 0.0535 - fmeasure: 0.9882 - val_loss: 0.0380 - val_fmeasure: 0.9883\n",
      "Epoch 91/256\n",
      "1s - loss: 0.0535 - fmeasure: 0.9882 - val_loss: 0.0380 - val_fmeasure: 0.9884\n",
      "Epoch 92/256\n",
      "1s - loss: 0.0534 - fmeasure: 0.9882 - val_loss: 0.0378 - val_fmeasure: 0.9882\n",
      "Epoch 93/256\n",
      "1s - loss: 0.0533 - fmeasure: 0.9882 - val_loss: 0.0377 - val_fmeasure: 0.9888\n",
      "Epoch 94/256\n",
      "1s - loss: 0.0532 - fmeasure: 0.9884 - val_loss: 0.0376 - val_fmeasure: 0.9882\n",
      "Epoch 95/256\n",
      "1s - loss: 0.0532 - fmeasure: 0.9884 - val_loss: 0.0376 - val_fmeasure: 0.9883\n",
      "Epoch 96/256\n",
      "1s - loss: 0.0531 - fmeasure: 0.9884 - val_loss: 0.0375 - val_fmeasure: 0.9881\n",
      "Epoch 97/256\n",
      "1s - loss: 0.0531 - fmeasure: 0.9884 - val_loss: 0.0375 - val_fmeasure: 0.9881\n",
      "Epoch 98/256\n",
      "1s - loss: 0.0530 - fmeasure: 0.9883 - val_loss: 0.0374 - val_fmeasure: 0.9883\n",
      "Epoch 99/256\n",
      "1s - loss: 0.0529 - fmeasure: 0.9884 - val_loss: 0.0375 - val_fmeasure: 0.9881\n",
      "Epoch 100/256\n",
      "1s - loss: 0.0529 - fmeasure: 0.9883 - val_loss: 0.0373 - val_fmeasure: 0.9884\n",
      "Epoch 101/256\n",
      "1s - loss: 0.0528 - fmeasure: 0.9884 - val_loss: 0.0373 - val_fmeasure: 0.9886\n",
      "Epoch 102/256\n",
      "1s - loss: 0.0528 - fmeasure: 0.9885 - val_loss: 0.0372 - val_fmeasure: 0.9884\n",
      "Epoch 103/256\n",
      "1s - loss: 0.0527 - fmeasure: 0.9886 - val_loss: 0.0371 - val_fmeasure: 0.9881\n",
      "Epoch 104/256\n",
      "1s - loss: 0.0527 - fmeasure: 0.9884 - val_loss: 0.0371 - val_fmeasure: 0.9884\n",
      "Epoch 105/256\n",
      "1s - loss: 0.0526 - fmeasure: 0.9884 - val_loss: 0.0370 - val_fmeasure: 0.9883\n",
      "Epoch 106/256\n",
      "1s - loss: 0.0526 - fmeasure: 0.9885 - val_loss: 0.0370 - val_fmeasure: 0.9884\n",
      "Epoch 107/256\n",
      "1s - loss: 0.0525 - fmeasure: 0.9886 - val_loss: 0.0369 - val_fmeasure: 0.9885\n",
      "Epoch 108/256\n",
      "1s - loss: 0.0524 - fmeasure: 0.9886 - val_loss: 0.0369 - val_fmeasure: 0.9884\n",
      "Epoch 109/256\n",
      "1s - loss: 0.0524 - fmeasure: 0.9886 - val_loss: 0.0370 - val_fmeasure: 0.9883\n",
      "Epoch 110/256\n",
      "1s - loss: 0.0524 - fmeasure: 0.9886 - val_loss: 0.0368 - val_fmeasure: 0.9886\n",
      "Epoch 111/256\n",
      "1s - loss: 0.0523 - fmeasure: 0.9888 - val_loss: 0.0368 - val_fmeasure: 0.9885\n",
      "Epoch 112/256\n",
      "1s - loss: 0.0522 - fmeasure: 0.9887 - val_loss: 0.0368 - val_fmeasure: 0.9888\n",
      "Epoch 113/256\n",
      "1s - loss: 0.0522 - fmeasure: 0.9887 - val_loss: 0.0367 - val_fmeasure: 0.9884\n",
      "Epoch 114/256\n",
      "1s - loss: 0.0521 - fmeasure: 0.9887 - val_loss: 0.0367 - val_fmeasure: 0.9886\n",
      "Epoch 115/256\n",
      "1s - loss: 0.0521 - fmeasure: 0.9888 - val_loss: 0.0367 - val_fmeasure: 0.9885\n",
      "Epoch 116/256\n",
      "1s - loss: 0.0520 - fmeasure: 0.9887 - val_loss: 0.0367 - val_fmeasure: 0.9884\n",
      "Epoch 117/256\n",
      "1s - loss: 0.0520 - fmeasure: 0.9887 - val_loss: 0.0366 - val_fmeasure: 0.9888\n",
      "Epoch 118/256\n",
      "1s - loss: 0.0520 - fmeasure: 0.9887 - val_loss: 0.0365 - val_fmeasure: 0.9889\n",
      "Epoch 119/256\n",
      "2s - loss: 0.0519 - fmeasure: 0.9889 - val_loss: 0.0366 - val_fmeasure: 0.9886\n",
      "Epoch 120/256\n",
      "1s - loss: 0.0519 - fmeasure: 0.9888 - val_loss: 0.0365 - val_fmeasure: 0.9887\n",
      "Epoch 121/256\n",
      "1s - loss: 0.0518 - fmeasure: 0.9888 - val_loss: 0.0363 - val_fmeasure: 0.9887\n",
      "Epoch 122/256\n",
      "2s - loss: 0.0518 - fmeasure: 0.9888 - val_loss: 0.0364 - val_fmeasure: 0.9888\n",
      "Epoch 123/256\n",
      "2s - loss: 0.0517 - fmeasure: 0.9888 - val_loss: 0.0363 - val_fmeasure: 0.9888\n",
      "Epoch 124/256\n",
      "1s - loss: 0.0517 - fmeasure: 0.9889 - val_loss: 0.0364 - val_fmeasure: 0.9884\n",
      "Epoch 125/256\n",
      "1s - loss: 0.0516 - fmeasure: 0.9889 - val_loss: 0.0362 - val_fmeasure: 0.9891\n",
      "Epoch 126/256\n",
      "2s - loss: 0.0516 - fmeasure: 0.9889 - val_loss: 0.0364 - val_fmeasure: 0.9891\n",
      "Epoch 127/256\n",
      "1s - loss: 0.0516 - fmeasure: 0.9890 - val_loss: 0.0362 - val_fmeasure: 0.9891\n",
      "Epoch 128/256\n",
      "1s - loss: 0.0515 - fmeasure: 0.9889 - val_loss: 0.0362 - val_fmeasure: 0.9889\n",
      "Epoch 129/256\n",
      "1s - loss: 0.0515 - fmeasure: 0.9890 - val_loss: 0.0360 - val_fmeasure: 0.9890\n",
      "Epoch 130/256\n",
      "1s - loss: 0.0514 - fmeasure: 0.9890 - val_loss: 0.0362 - val_fmeasure: 0.9890\n",
      "Epoch 131/256\n",
      "2s - loss: 0.0514 - fmeasure: 0.9890 - val_loss: 0.0360 - val_fmeasure: 0.9890\n",
      "Epoch 132/256\n",
      "1s - loss: 0.0513 - fmeasure: 0.9890 - val_loss: 0.0360 - val_fmeasure: 0.9888\n",
      "Epoch 133/256\n",
      "1s - loss: 0.0513 - fmeasure: 0.9890 - val_loss: 0.0360 - val_fmeasure: 0.9888\n",
      "Epoch 134/256\n",
      "1s - loss: 0.0513 - fmeasure: 0.9889 - val_loss: 0.0361 - val_fmeasure: 0.9888\n",
      "Epoch 135/256\n",
      "1s - loss: 0.0512 - fmeasure: 0.9890 - val_loss: 0.0360 - val_fmeasure: 0.9888\n",
      "Epoch 136/256\n",
      "1s - loss: 0.0512 - fmeasure: 0.9890 - val_loss: 0.0358 - val_fmeasure: 0.9889\n",
      "Epoch 137/256\n",
      "1s - loss: 0.0511 - fmeasure: 0.9891 - val_loss: 0.0359 - val_fmeasure: 0.9888\n",
      "Epoch 138/256\n",
      "1s - loss: 0.0511 - fmeasure: 0.9891 - val_loss: 0.0359 - val_fmeasure: 0.9886\n",
      "Epoch 139/256\n",
      "1s - loss: 0.0511 - fmeasure: 0.9891 - val_loss: 0.0357 - val_fmeasure: 0.9889\n",
      "Epoch 140/256\n",
      "1s - loss: 0.0510 - fmeasure: 0.9891 - val_loss: 0.0357 - val_fmeasure: 0.9890\n",
      "Epoch 141/256\n",
      "1s - loss: 0.0510 - fmeasure: 0.9892 - val_loss: 0.0359 - val_fmeasure: 0.9892\n",
      "Epoch 142/256\n",
      "1s - loss: 0.0510 - fmeasure: 0.9891 - val_loss: 0.0357 - val_fmeasure: 0.9888\n",
      "Epoch 143/256\n",
      "1s - loss: 0.0509 - fmeasure: 0.9892 - val_loss: 0.0357 - val_fmeasure: 0.9890\n",
      "Epoch 144/256\n",
      "1s - loss: 0.0509 - fmeasure: 0.9892 - val_loss: 0.0356 - val_fmeasure: 0.9889\n",
      "Epoch 145/256\n",
      "1s - loss: 0.0509 - fmeasure: 0.9892 - val_loss: 0.0356 - val_fmeasure: 0.9892\n",
      "Epoch 146/256\n",
      "1s - loss: 0.0508 - fmeasure: 0.9891 - val_loss: 0.0356 - val_fmeasure: 0.9892\n",
      "Epoch 147/256\n",
      "1s - loss: 0.0508 - fmeasure: 0.9892 - val_loss: 0.0355 - val_fmeasure: 0.9892\n",
      "Epoch 148/256\n",
      "1s - loss: 0.0507 - fmeasure: 0.9892 - val_loss: 0.0355 - val_fmeasure: 0.9893\n",
      "Epoch 149/256\n",
      "1s - loss: 0.0507 - fmeasure: 0.9893 - val_loss: 0.0355 - val_fmeasure: 0.9890\n",
      "Epoch 150/256\n",
      "1s - loss: 0.0507 - fmeasure: 0.9893 - val_loss: 0.0354 - val_fmeasure: 0.9889\n",
      "Epoch 151/256\n",
      "1s - loss: 0.0506 - fmeasure: 0.9892 - val_loss: 0.0354 - val_fmeasure: 0.9891\n",
      "Epoch 152/256\n",
      "1s - loss: 0.0506 - fmeasure: 0.9893 - val_loss: 0.0354 - val_fmeasure: 0.9895\n",
      "Epoch 153/256\n",
      "1s - loss: 0.0506 - fmeasure: 0.9892 - val_loss: 0.0354 - val_fmeasure: 0.9893\n",
      "Epoch 154/256\n",
      "1s - loss: 0.0505 - fmeasure: 0.9893 - val_loss: 0.0354 - val_fmeasure: 0.9894\n",
      "Epoch 155/256\n",
      "1s - loss: 0.0505 - fmeasure: 0.9893 - val_loss: 0.0352 - val_fmeasure: 0.9890\n",
      "Epoch 156/256\n",
      "1s - loss: 0.0505 - fmeasure: 0.9893 - val_loss: 0.0354 - val_fmeasure: 0.9893\n",
      "Epoch 157/256\n",
      "1s - loss: 0.0504 - fmeasure: 0.9894 - val_loss: 0.0352 - val_fmeasure: 0.9891\n",
      "Epoch 158/256\n",
      "1s - loss: 0.0504 - fmeasure: 0.9894 - val_loss: 0.0352 - val_fmeasure: 0.9889\n",
      "Epoch 159/256\n",
      "1s - loss: 0.0504 - fmeasure: 0.9893 - val_loss: 0.0352 - val_fmeasure: 0.9892\n",
      "Epoch 160/256\n",
      "1s - loss: 0.0504 - fmeasure: 0.9894 - val_loss: 0.0352 - val_fmeasure: 0.9894\n",
      "Epoch 161/256\n",
      "1s - loss: 0.0503 - fmeasure: 0.9895 - val_loss: 0.0352 - val_fmeasure: 0.9893\n",
      "Epoch 162/256\n",
      "1s - loss: 0.0503 - fmeasure: 0.9895 - val_loss: 0.0352 - val_fmeasure: 0.9894\n",
      "Epoch 163/256\n",
      "1s - loss: 0.0502 - fmeasure: 0.9894 - val_loss: 0.0351 - val_fmeasure: 0.9894\n",
      "Epoch 164/256\n",
      "1s - loss: 0.0502 - fmeasure: 0.9894 - val_loss: 0.0351 - val_fmeasure: 0.9893\n",
      "Epoch 165/256\n",
      "1s - loss: 0.0502 - fmeasure: 0.9894 - val_loss: 0.0351 - val_fmeasure: 0.9893\n",
      "Epoch 166/256\n",
      "1s - loss: 0.0502 - fmeasure: 0.9894 - val_loss: 0.0350 - val_fmeasure: 0.9892\n",
      "Epoch 167/256\n",
      "1s - loss: 0.0501 - fmeasure: 0.9894 - val_loss: 0.0350 - val_fmeasure: 0.9894\n",
      "Epoch 168/256\n",
      "1s - loss: 0.0501 - fmeasure: 0.9895 - val_loss: 0.0349 - val_fmeasure: 0.9893\n",
      "Epoch 169/256\n",
      "1s - loss: 0.0501 - fmeasure: 0.9895 - val_loss: 0.0349 - val_fmeasure: 0.9892\n",
      "Epoch 170/256\n",
      "1s - loss: 0.0500 - fmeasure: 0.9895 - val_loss: 0.0349 - val_fmeasure: 0.9894\n",
      "Epoch 171/256\n",
      "1s - loss: 0.0500 - fmeasure: 0.9895 - val_loss: 0.0350 - val_fmeasure: 0.9894\n",
      "Epoch 172/256\n",
      "1s - loss: 0.0500 - fmeasure: 0.9894 - val_loss: 0.0349 - val_fmeasure: 0.9896\n",
      "Epoch 173/256\n",
      "1s - loss: 0.0500 - fmeasure: 0.9895 - val_loss: 0.0349 - val_fmeasure: 0.9895\n",
      "Epoch 174/256\n",
      "1s - loss: 0.0499 - fmeasure: 0.9895 - val_loss: 0.0349 - val_fmeasure: 0.9893\n",
      "Epoch 175/256\n",
      "1s - loss: 0.0499 - fmeasure: 0.9895 - val_loss: 0.0348 - val_fmeasure: 0.9894\n",
      "Epoch 176/256\n",
      "1s - loss: 0.0499 - fmeasure: 0.9896 - val_loss: 0.0348 - val_fmeasure: 0.9893\n",
      "Epoch 177/256\n",
      "1s - loss: 0.0498 - fmeasure: 0.9895 - val_loss: 0.0348 - val_fmeasure: 0.9895\n",
      "Epoch 178/256\n",
      "1s - loss: 0.0498 - fmeasure: 0.9895 - val_loss: 0.0347 - val_fmeasure: 0.9895\n",
      "Epoch 179/256\n",
      "1s - loss: 0.0498 - fmeasure: 0.9895 - val_loss: 0.0348 - val_fmeasure: 0.9894\n",
      "Epoch 180/256\n",
      "1s - loss: 0.0498 - fmeasure: 0.9895 - val_loss: 0.0347 - val_fmeasure: 0.9895\n",
      "Epoch 181/256\n",
      "1s - loss: 0.0497 - fmeasure: 0.9896 - val_loss: 0.0347 - val_fmeasure: 0.9896\n",
      "Epoch 182/256\n",
      "1s - loss: 0.0497 - fmeasure: 0.9896 - val_loss: 0.0346 - val_fmeasure: 0.9893\n",
      "Epoch 183/256\n",
      "1s - loss: 0.0497 - fmeasure: 0.9896 - val_loss: 0.0347 - val_fmeasure: 0.9896\n",
      "Epoch 184/256\n",
      "1s - loss: 0.0497 - fmeasure: 0.9895 - val_loss: 0.0346 - val_fmeasure: 0.9895\n",
      "Epoch 185/256\n",
      "1s - loss: 0.0496 - fmeasure: 0.9895 - val_loss: 0.0346 - val_fmeasure: 0.9898\n",
      "Epoch 186/256\n",
      "1s - loss: 0.0496 - fmeasure: 0.9897 - val_loss: 0.0346 - val_fmeasure: 0.9894\n",
      "Epoch 187/256\n",
      "1s - loss: 0.0496 - fmeasure: 0.9896 - val_loss: 0.0345 - val_fmeasure: 0.9894\n",
      "Epoch 188/256\n",
      "1s - loss: 0.0496 - fmeasure: 0.9895 - val_loss: 0.0346 - val_fmeasure: 0.9898\n",
      "Epoch 189/256\n",
      "1s - loss: 0.0495 - fmeasure: 0.9896 - val_loss: 0.0345 - val_fmeasure: 0.9898\n",
      "Epoch 190/256\n",
      "1s - loss: 0.0495 - fmeasure: 0.9897 - val_loss: 0.0345 - val_fmeasure: 0.9895\n",
      "Epoch 191/256\n",
      "1s - loss: 0.0495 - fmeasure: 0.9896 - val_loss: 0.0344 - val_fmeasure: 0.9895\n",
      "Epoch 192/256\n",
      "1s - loss: 0.0495 - fmeasure: 0.9897 - val_loss: 0.0344 - val_fmeasure: 0.9895\n",
      "Epoch 193/256\n",
      "1s - loss: 0.0494 - fmeasure: 0.9897 - val_loss: 0.0344 - val_fmeasure: 0.9897\n",
      "Epoch 194/256\n",
      "1s - loss: 0.0494 - fmeasure: 0.9896 - val_loss: 0.0344 - val_fmeasure: 0.9896\n",
      "Epoch 195/256\n",
      "1s - loss: 0.0494 - fmeasure: 0.9898 - val_loss: 0.0344 - val_fmeasure: 0.9897\n",
      "Epoch 196/256\n",
      "1s - loss: 0.0494 - fmeasure: 0.9897 - val_loss: 0.0343 - val_fmeasure: 0.9896\n",
      "Epoch 197/256\n",
      "1s - loss: 0.0493 - fmeasure: 0.9897 - val_loss: 0.0343 - val_fmeasure: 0.9897\n",
      "Epoch 198/256\n",
      "1s - loss: 0.0493 - fmeasure: 0.9897 - val_loss: 0.0344 - val_fmeasure: 0.9896\n",
      "Epoch 199/256\n",
      "1s - loss: 0.0493 - fmeasure: 0.9897 - val_loss: 0.0343 - val_fmeasure: 0.9895\n",
      "Epoch 200/256\n",
      "1s - loss: 0.0493 - fmeasure: 0.9898 - val_loss: 0.0343 - val_fmeasure: 0.9898\n",
      "Epoch 201/256\n",
      "1s - loss: 0.0492 - fmeasure: 0.9898 - val_loss: 0.0343 - val_fmeasure: 0.9897\n",
      "Epoch 202/256\n",
      "1s - loss: 0.0492 - fmeasure: 0.9898 - val_loss: 0.0343 - val_fmeasure: 0.9897\n",
      "Epoch 203/256\n",
      "1s - loss: 0.0492 - fmeasure: 0.9898 - val_loss: 0.0342 - val_fmeasure: 0.9897\n",
      "Epoch 204/256\n",
      "1s - loss: 0.0492 - fmeasure: 0.9898 - val_loss: 0.0342 - val_fmeasure: 0.9898\n",
      "Epoch 205/256\n",
      "1s - loss: 0.0492 - fmeasure: 0.9897 - val_loss: 0.0342 - val_fmeasure: 0.9898\n",
      "Epoch 206/256\n",
      "1s - loss: 0.0491 - fmeasure: 0.9898 - val_loss: 0.0342 - val_fmeasure: 0.9898\n",
      "Epoch 207/256\n",
      "1s - loss: 0.0491 - fmeasure: 0.9899 - val_loss: 0.0342 - val_fmeasure: 0.9898\n",
      "Epoch 208/256\n",
      "1s - loss: 0.0491 - fmeasure: 0.9898 - val_loss: 0.0341 - val_fmeasure: 0.9898\n",
      "Epoch 209/256\n",
      "1s - loss: 0.0491 - fmeasure: 0.9899 - val_loss: 0.0342 - val_fmeasure: 0.9899\n",
      "Epoch 210/256\n",
      "1s - loss: 0.0490 - fmeasure: 0.9898 - val_loss: 0.0341 - val_fmeasure: 0.9899\n",
      "Epoch 211/256\n",
      "1s - loss: 0.0490 - fmeasure: 0.9898 - val_loss: 0.0340 - val_fmeasure: 0.9898\n",
      "Epoch 212/256\n",
      "1s - loss: 0.0490 - fmeasure: 0.9898 - val_loss: 0.0340 - val_fmeasure: 0.9896\n",
      "Epoch 213/256\n",
      "1s - loss: 0.0490 - fmeasure: 0.9898 - val_loss: 0.0341 - val_fmeasure: 0.9897\n",
      "Epoch 214/256\n",
      "1s - loss: 0.0489 - fmeasure: 0.9899 - val_loss: 0.0341 - val_fmeasure: 0.9898\n",
      "Epoch 215/256\n",
      "1s - loss: 0.0489 - fmeasure: 0.9898 - val_loss: 0.0340 - val_fmeasure: 0.9899\n",
      "Epoch 216/256\n",
      "1s - loss: 0.0489 - fmeasure: 0.9898 - val_loss: 0.0340 - val_fmeasure: 0.9897\n",
      "Epoch 217/256\n",
      "1s - loss: 0.0489 - fmeasure: 0.9900 - val_loss: 0.0340 - val_fmeasure: 0.9899\n",
      "Epoch 218/256\n",
      "1s - loss: 0.0489 - fmeasure: 0.9899 - val_loss: 0.0340 - val_fmeasure: 0.9899\n",
      "Epoch 219/256\n",
      "1s - loss: 0.0489 - fmeasure: 0.9899 - val_loss: 0.0340 - val_fmeasure: 0.9899\n",
      "Epoch 220/256\n",
      "1s - loss: 0.0488 - fmeasure: 0.9899 - val_loss: 0.0339 - val_fmeasure: 0.9900\n",
      "Epoch 221/256\n",
      "1s - loss: 0.0488 - fmeasure: 0.9900 - val_loss: 0.0339 - val_fmeasure: 0.9899\n",
      "Epoch 222/256\n",
      "1s - loss: 0.0488 - fmeasure: 0.9899 - val_loss: 0.0339 - val_fmeasure: 0.9900\n",
      "Epoch 223/256\n",
      "1s - loss: 0.0488 - fmeasure: 0.9899 - val_loss: 0.0339 - val_fmeasure: 0.9898\n",
      "Epoch 224/256\n",
      "1s - loss: 0.0487 - fmeasure: 0.9899 - val_loss: 0.0339 - val_fmeasure: 0.9901\n",
      "Epoch 225/256\n",
      "1s - loss: 0.0487 - fmeasure: 0.9899 - val_loss: 0.0338 - val_fmeasure: 0.9899\n",
      "Epoch 226/256\n",
      "1s - loss: 0.0487 - fmeasure: 0.9899 - val_loss: 0.0338 - val_fmeasure: 0.9900\n",
      "Epoch 227/256\n",
      "1s - loss: 0.0487 - fmeasure: 0.9899 - val_loss: 0.0338 - val_fmeasure: 0.9901\n",
      "Epoch 228/256\n",
      "1s - loss: 0.0487 - fmeasure: 0.9900 - val_loss: 0.0338 - val_fmeasure: 0.9899\n",
      "Epoch 229/256\n",
      "1s - loss: 0.0487 - fmeasure: 0.9899 - val_loss: 0.0338 - val_fmeasure: 0.9898\n",
      "Epoch 230/256\n",
      "1s - loss: 0.0486 - fmeasure: 0.9900 - val_loss: 0.0338 - val_fmeasure: 0.9899\n",
      "Epoch 231/256\n",
      "1s - loss: 0.0486 - fmeasure: 0.9899 - val_loss: 0.0337 - val_fmeasure: 0.9901\n",
      "Epoch 232/256\n",
      "1s - loss: 0.0486 - fmeasure: 0.9900 - val_loss: 0.0338 - val_fmeasure: 0.9901\n",
      "Epoch 233/256\n",
      "1s - loss: 0.0486 - fmeasure: 0.9901 - val_loss: 0.0337 - val_fmeasure: 0.9900\n",
      "Epoch 234/256\n",
      "1s - loss: 0.0486 - fmeasure: 0.9899 - val_loss: 0.0337 - val_fmeasure: 0.9899\n",
      "Epoch 235/256\n",
      "1s - loss: 0.0485 - fmeasure: 0.9900 - val_loss: 0.0337 - val_fmeasure: 0.9900\n",
      "Epoch 236/256\n",
      "1s - loss: 0.0485 - fmeasure: 0.9900 - val_loss: 0.0337 - val_fmeasure: 0.9900\n",
      "Epoch 237/256\n",
      "1s - loss: 0.0485 - fmeasure: 0.9901 - val_loss: 0.0337 - val_fmeasure: 0.9899\n",
      "Epoch 238/256\n",
      "1s - loss: 0.0485 - fmeasure: 0.9899 - val_loss: 0.0336 - val_fmeasure: 0.9901\n",
      "Epoch 239/256\n",
      "1s - loss: 0.0485 - fmeasure: 0.9901 - val_loss: 0.0337 - val_fmeasure: 0.9899\n",
      "Epoch 240/256\n",
      "1s - loss: 0.0485 - fmeasure: 0.9900 - val_loss: 0.0336 - val_fmeasure: 0.9900\n",
      "Epoch 241/256\n",
      "1s - loss: 0.0484 - fmeasure: 0.9900 - val_loss: 0.0336 - val_fmeasure: 0.9900\n",
      "Epoch 242/256\n",
      "1s - loss: 0.0484 - fmeasure: 0.9901 - val_loss: 0.0336 - val_fmeasure: 0.9901\n",
      "Epoch 243/256\n",
      "1s - loss: 0.0484 - fmeasure: 0.9901 - val_loss: 0.0336 - val_fmeasure: 0.9901\n",
      "Epoch 244/256\n",
      "1s - loss: 0.0484 - fmeasure: 0.9900 - val_loss: 0.0335 - val_fmeasure: 0.9899\n",
      "Epoch 245/256\n",
      "1s - loss: 0.0484 - fmeasure: 0.9900 - val_loss: 0.0335 - val_fmeasure: 0.9901\n",
      "Epoch 246/256\n",
      "1s - loss: 0.0483 - fmeasure: 0.9901 - val_loss: 0.0335 - val_fmeasure: 0.9902\n",
      "Epoch 247/256\n",
      "1s - loss: 0.0483 - fmeasure: 0.9901 - val_loss: 0.0335 - val_fmeasure: 0.9901\n",
      "Epoch 248/256\n",
      "1s - loss: 0.0483 - fmeasure: 0.9902 - val_loss: 0.0336 - val_fmeasure: 0.9901\n",
      "Epoch 249/256\n",
      "1s - loss: 0.0483 - fmeasure: 0.9902 - val_loss: 0.0335 - val_fmeasure: 0.9902\n",
      "Epoch 250/256\n",
      "1s - loss: 0.0483 - fmeasure: 0.9901 - val_loss: 0.0335 - val_fmeasure: 0.9902\n",
      "Epoch 251/256\n",
      "1s - loss: 0.0483 - fmeasure: 0.9901 - val_loss: 0.0334 - val_fmeasure: 0.9903\n",
      "Epoch 252/256\n",
      "1s - loss: 0.0482 - fmeasure: 0.9901 - val_loss: 0.0334 - val_fmeasure: 0.9903\n",
      "Epoch 253/256\n",
      "1s - loss: 0.0482 - fmeasure: 0.9901 - val_loss: 0.0335 - val_fmeasure: 0.9904\n",
      "Epoch 254/256\n",
      "1s - loss: 0.0482 - fmeasure: 0.9901 - val_loss: 0.0334 - val_fmeasure: 0.9904\n",
      "Epoch 255/256\n",
      "1s - loss: 0.0482 - fmeasure: 0.9901 - val_loss: 0.0335 - val_fmeasure: 0.9901\n",
      "Epoch 256/256\n",
      "1s - loss: 0.0482 - fmeasure: 0.9902 - val_loss: 0.0334 - val_fmeasure: 0.9901\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, nb_epoch = 256, batch_size = 750, verbose = 2, \n",
    "                    validation_split = .20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our neural network achieves an f1 score of  0.990465951849 on the oversampled testing data\n",
      "[[52515   601]\n",
      " [  410 53094]]\n",
      "Our neural network achieves an f1 score of 0.993672767421 on the original testing data\n",
      "[[70200   876]\n",
      " [   18   108]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions[:] = predictions[:]>0.5\n",
    "\n",
    "print \"Our neural network achieves an f1 score of \" , f1_score(y_test, predictions, pos_label = 0) , \"on the oversampled testing data\"\n",
    "print confusion_matrix(y_test, predictions)\n",
    "\n",
    "opredictions = model.predict(test_features.values)\n",
    "opredictions[:] = opredictions[:]>0.5\n",
    "print \"Our neural network achieves an f1 score of\" , f1_score(test_labels.values, opredictions, pos_label = 0) , \"on the original testing data\"\n",
    "print confusion_matrix(test_labels.values, opredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do: Discuss these results; Looks like tuned random forest does better on oversampled data but neural network performs better on the original (non-oversampled) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEZCAYAAAC5AHPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQJAiwABoEWAANDyDVjhaX+lK9LEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2abafef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss history\n",
    "\n",
    "# Training for less epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('binary crossentropy loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training loss', 'validation loss'], loc = 'upper right')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
